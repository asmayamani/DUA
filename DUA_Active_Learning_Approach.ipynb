{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asmayamani/DUA/blob/main/DUA_Active_Learning_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjL7iUe8DI0b"
      },
      "source": [
        "#Active Learning Selection of training file\n",
        "\n",
        "Active learning originaly aims to maximize the information gained from labeling at a limited annotation budget. In this applictaion, as we noticed the classes are highley imbalanced, and the presence of images and large objects all problems thata could hender the model's ability to generalize at deployment.\n",
        "\n",
        "In this Notebook we present the Diversity in Uncertinity Aggregation (DUA) Active learning approach in combination with Weighting by Random Sample Performance (WRP).\n",
        "\n",
        "\n",
        "\n",
        "1.   It first evaluate the model from the previous iteration on a random subset of the training data that hasn't been used for trianing yet.\n",
        "2.   It then calculated the uncertinity of the inference made on the training data that hasn't been used for trianing yet, by summing the average uncertinities per class for each image\n",
        "3.  After that it multiplies the weights with uncertinities and produce the final uncertinity score per image.\n",
        "4. Finally, the images with the highest uncertinities are included in the new training cycle.\n",
        "5. The model is build using the selected images using the notebook in [here](https://github.com/asmayamani/DUA/blob/main/Active_Learning_Training.ipynb)\n",
        "\n",
        "It assumes an initial model and initial labled pool\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRsOwsQRh8-U"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "587aXRGvkO-u",
        "outputId": "14d0a7df-8c34-4115-969d-270e1155fd79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount= True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8w8GYzIl5PY"
      },
      "outputs": [],
      "source": [
        "import os, glob, random, shutil\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/asmayamani/yolov7-AL\n",
        "%cd yolov7-AL\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "da0zjHgt9QjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H99K5e0QDxWk"
      },
      "source": [
        "#Declerations\n",
        "\n",
        "Important: Increase the itereation after completing each iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1fL5Av4ijJN"
      },
      "outputs": [],
      "source": [
        "#Enter the number iteration\n",
        "iteration = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKBC1a7osjJ4"
      },
      "outputs": [],
      "source": [
        "main_dir = '/content/drive/MyDrive/AL/VOC2012/'\n",
        "complete_data_folder = main_dir+'TrainF/'\n",
        "main_exp_dir = 'DUA500/'\n",
        "iteration_folder = 'Iteration_'+str(iteration)+'/'\n",
        "complete_iteration_folder = main_dir+main_exp_dir+iteration_folder\n",
        "os.makedirs(complete_iteration_folder) if not os.path.exists(complete_iteration_folder) else None\n",
        "object_def = {'none':0,'aeroplane':1,'bicycle':2,'bird':3,'boat':4,'bottle':5,'bus':6,'car':7,'cat':8,'chair':9,'cow':10,'diningtable':11,\n",
        "    'dog':12,'horse':13,'motorbike':14,'person':15,'pottedplant':16,'sheep':17,'sofa':18,'train':19,'tvmonitor':20}\n",
        "approach = 'DUA'\n",
        "weighting_approach = 'WRP'\n",
        "number_of_random_images_from_training = 5000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umQak1WeN-SD"
      },
      "outputs": [],
      "source": [
        "output_model = approach+str(iteration)+str(number_of_random_images_from_training)+str(weighting_approach)\n",
        "previous_model_zip = 'Iteration_'+str(iteration-1)+'_Training_whole_best.zip'\n",
        "budgetPerIteration = 500\n",
        "expName = 'iteration'+str(iteration)+approach+'_'+str(number_of_random_images_from_training)\n",
        "wexpName = 'w_iteration'+str(iteration)+approach+'_'+str(number_of_random_images_from_training)\n",
        "previousExpfName = 'iteration'+str(iteration-1)+approach+weighting_approach+'_'+str(number_of_random_images_from_training)+'/'\n",
        "previous_iteration_folder = 'Iteration_'+str(iteration-1)+'/'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXbq0FbwDkpJ"
      },
      "source": [
        "#Definitions\n",
        "\n",
        "The methods for active learning and some utility functions are defined here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kTDDNI-l8tQ"
      },
      "outputs": [],
      "source": [
        "def DUA(fDir):\n",
        "    classes = {}\n",
        "    for i in range(len(object_def)):\n",
        "      classes[str(i)] = {'objectsC' : 0, 'uncer' : 0}\n",
        "    try:\n",
        "      with open(fDir, 'r') as file2:  # read annotation.txt\n",
        "          for row in [x.split(' ') for x in file2.read().strip().splitlines()]:\n",
        "              classes[str(row[0])]['objectsC'] += 1\n",
        "              classes[str(row[0])]['uncer'] += ((1-float(row[5])))\n",
        "      file2.close()\n",
        "      total = 0\n",
        "      wAvg = 0\n",
        "      for i in range(len(object_def)):\n",
        "        # print(classes[str(i)])\n",
        "        if classes[str(i)]['objectsC'] != 0:\n",
        "          wAvg += ((classes[str(i)]['uncer']/classes[str(i)]['objectsC']) * classesWeight[str(i)])\n",
        "          total += classes[str(i)]['objectsC']\n",
        "\n",
        "      return(wAvg)\n",
        "    except (IOError, OSError) as e:\n",
        "        print(e)\n",
        "        return(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O63-VklkmDiW"
      },
      "outputs": [],
      "source": [
        "def getUncertinitiesForDir(dir, trainDir = [], valAccuracyDirF = '', method = 'avg', weighting = 1 , budget = 100):\n",
        "  classesWeight = getWeightsForClassses(weighting, trainDir = trainDir, valAccuracyDirF = valAccuracyDirF)\n",
        "  listofFilesNamesUncertinity = {}\n",
        "  for f in glob.glob(dir+'/labels/*.txt'):\n",
        "    namef = Path(f).name[:-4]\n",
        "    fileFullDir = dir+'/labels/'+namef+'.txt'\n",
        "    if method == 'DUA':\n",
        "      listofFilesNamesUncertinity[namef] = DUA(fileFullDir,classesWeight)\n",
        "  print(listofFilesNamesUncertinity)\n",
        "  return sorted(listofFilesNamesUncertinity, key=listofFilesNamesUncertinity.get, reverse=True)[:budget]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGgfPevXmF5z"
      },
      "outputs": [],
      "source": [
        "def getWeightsForClassses(weighting, trainDir = [], valAccuracyDirF = '', ):\n",
        "  classesWeight = {}\n",
        "  if weighting == 1:\n",
        "    for i in range(len(object_def)):\n",
        "      classesWeight[str(i)] = 1\n",
        "  elif weighting == 3:\n",
        "    classesWeight = applyMinMaxScalerToW(getWeightForValAccuracy(valAccuracyDirF))\n",
        "  print(classesWeight)\n",
        "  return classesWeight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfNnhpiYmLxJ"
      },
      "outputs": [],
      "source": [
        "def newTraining(namefolder,setOfFileNames,oldTraning):\n",
        "  added = 0\n",
        "  path_label_2 = complete_iteration_folder+namefolder+'/labels/'\n",
        "  path_images_2 = complete_iteration_folder+namefolder+'/images/'\n",
        "  os.makedirs(path_label_2) if not os.path.exists(path_label_2) else None\n",
        "  os.makedirs(path_images_2) if not os.path.exists(path_images_2) else None\n",
        "  count = 0\n",
        "\n",
        "  while (added < budgetPerIteration) and (count < len(setOfFileNames)):\n",
        "    namef = setOfFileNames[count] #Path(setOfFileNames[count]).name[:-4]\n",
        "\n",
        "    l_src_path = complete_data_folder+'labels/'+ namef+'.txt'\n",
        "    i_src_path = complete_data_folder+'images/'+ namef+'.jpg'\n",
        "\n",
        "    src_path = complete_data_folder+'labels/'+ namef+'.txt'\n",
        "    if os.path.exists(l_src_path) and os.path.exists(i_src_path):\n",
        "      dst_path = path_label_2 + namef+'.txt'\n",
        "      shutil.copy(src_path, dst_path)\n",
        "      src_path = complete_data_folder+'images/'+ namef+'.jpg'\n",
        "      dst_path = path_images_2 + namef+'.jpg'\n",
        "      shutil.copy(src_path, dst_path)\n",
        "      added += 1\n",
        "    count += 1\n",
        "\n",
        "  for pth in oldTraning:\n",
        "    # print(pth)\n",
        "    namef = Path(pth).name[:-4]\n",
        "    src_path = main_dir+main_exp_dir+previous_iteration_folder+previousExpfName+'labels/'+ namef+'.txt'\n",
        "    dst_path = path_label_2 + namef+'.txt'\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "    src_path = main_dir+main_exp_dir+previous_iteration_folder+previousExpfName+'images/'+ namef+'.jpg'\n",
        "    dst_path = path_images_2 + namef+'.jpg'\n",
        "    shutil.copy(src_path, dst_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlvWpYaamRYI"
      },
      "outputs": [],
      "source": [
        "def getWeightForValAccuracy(fDir, cnum = object_def):\n",
        "    numberOfClasses = len(object_def)\n",
        "    accuracyClasses = {}\n",
        "    for i in range(numberOfClasses):\n",
        "      accuracyClasses[str(i)] = 0\n",
        "    try:\n",
        "      with open(fDir, 'r') as file2:  # read annotation.txt\n",
        "          first = True\n",
        "          for row in [x.split(' ') for x in file2.read().strip().splitlines()]:\n",
        "            if first:\n",
        "              first = False\n",
        "              continue\n",
        "            if row == '':\n",
        "              continue\n",
        "            row = list(filter(lambda a: a != '', row))\n",
        "            accuracyClasses[str(cnum[row[0]])] = float(row[5])\n",
        "\n",
        "\n",
        "      file2.close()\n",
        "      return(accuracyClasses)\n",
        "    except (IOError, OSError) as e:\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tiIVmx6mUJy"
      },
      "outputs": [],
      "source": [
        "def applyMinMaxScalerToW (wValCDic):\n",
        "  minV = min(wValCDic[item] for item in wValCDic)\n",
        "  maxV = max(wValCDic[item] for item in wValCDic)\n",
        "  for i in range(len(object_def)):\n",
        "    wValCDic[str(i)] = 1- ((wValCDic[str(i)] - minV)/(maxV - minV))\n",
        "  return wValCDic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbQf12tsmUz_"
      },
      "outputs": [],
      "source": [
        "def nominateImagesToObtainUncer(wasTrainedOnDir, experimentName, forWeights = False, fromRandSize = 500, seed = 42, trainingDir = complete_data_folder+'images/', weightedOnDir = '' ):\n",
        "  #remove images that were trained on\n",
        "  trainedOn = glob.glob(wasTrainedOnDir+'images/*.jpg')\n",
        "  print(len(trainedOn))\n",
        "  fullTrainSet = glob.glob(trainingDir+'*.jpg')\n",
        "  orpNames = [Path(f).name[:-4] for f in trainedOn]\n",
        "  if weightedOnDir != '':\n",
        "    weightedOn = glob.glob(weightedOnDir+'/images/*.jpg')\n",
        "    orpNames = orpNames + [Path(f).name[:-4] for f in weightedOn]\n",
        "\n",
        "  mixedRand = []\n",
        "  for ele in fullTrainSet:\n",
        "    for ele2 in orpNames:\n",
        "      if ele2 in ele:\n",
        "        mixedRand.append(ele)\n",
        "        continue\n",
        "  randomPathes2 = [ele for ele in fullTrainSet if ele not in mixedRand]\n",
        "\n",
        "  reducedRandomPathes2 = []\n",
        "  for rp2 in randomPathes2:\n",
        "    if os.path.exists(complete_data_folder+'labels/'+ Path(rp2).name[:-4] + '.txt'):\n",
        "        reducedRandomPathes2.append(rp2)\n",
        "  toGetUncertinityPath = reducedRandomPathes2\n",
        "  print(len(toGetUncertinityPath))\n",
        "  if fromRandSize < len(toGetUncertinityPath):\n",
        "    toGetUncertinityPath = random.sample(reducedRandomPathes2, k=fromRandSize)\n",
        "  print(len(toGetUncertinityPath))\n",
        "\n",
        "  newFolderName = experimentName+'_Nominees'\n",
        "  path_label = complete_iteration_folder+newFolderName+'/labels/'\n",
        "  path_images = complete_iteration_folder+newFolderName+'/images/'\n",
        "  os.makedirs(path_label) if not os.path.exists(path_label) else None\n",
        "  os.makedirs(path_images) if not os.path.exists(path_images) else None\n",
        "\n",
        "  for pth in toGetUncertinityPath:\n",
        "    namef = Path(pth).name[:-4]\n",
        "    if forWeights:\n",
        "      src_path = complete_data_folder+'/labels/'+ namef+'.txt'\n",
        "      dst_path = path_label + namef+'.txt'\n",
        "      shutil.copy(src_path, dst_path)\n",
        "    src_path = complete_data_folder+'/images/'+ namef+'.jpg'\n",
        "    dst_path = path_images + namef+'.jpg'\n",
        "    shutil.copy(src_path, dst_path)\n",
        "  return path_images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOAJNyE0EM2d"
      },
      "source": [
        "#Test on Random subset\n",
        "In this part we select some random validation subset to evaluate the peformance on and subsequently weigh the uncertinitiies of the different classes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_images_nominees_test = nominateImagesToObtainUncer(main_dir+main_exp_dir+previous_iteration_folder+previousExpfName,wexpName,fromRandSize=int(budgetPerIteration*0.1),forWeights=True)"
      ],
      "metadata": {
        "id": "V9naKLO-90rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip {main_dir+main_exp_dir+previous_iteration_folder}{previous_model_zip} -d /content/yolov7-AL/{previous_iteration_folder}"
      ],
      "metadata": {
        "id": "6Mh51WcF94Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiPcsr7wmhU4"
      },
      "outputs": [],
      "source": [
        "#This somtims has to be updated manually based on the unzipped file above\n",
        "weights_path = previous_iteration_folder+'/content/runs/detect/train/weights/best.pt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "config = {\n",
        "         'path': '../datasets/VisDrone',\n",
        "         'train': '/content/drive/MyDrive/AL/VOC2012/ValF/images',\n",
        "         'val': '/content/drive/MyDrive/AL/VOC2012/DUA-WRP500/Iteration_6/w_iteration6DUA_5000_Nominees',\n",
        "         'nc': 21,\n",
        "         'names': [\n",
        "    'none',\n",
        "    'aeroplane',\n",
        "    'bicycle',\n",
        "    'bird',\n",
        "    'boat',\n",
        "    'bottle',\n",
        "    'bus',\n",
        "    'car',\n",
        "    'cat',\n",
        "    'chair',\n",
        "    'cow',\n",
        "    'diningtable',\n",
        "    'dog',\n",
        "    'horse',\n",
        "    'motorbike',\n",
        "    'person',\n",
        "    'pottedplant',\n",
        "    'sheep',\n",
        "    'sofa',\n",
        "    'train',\n",
        "    'tvmonitor'\n",
        "]}\n",
        "\n",
        "with open(\"data.yaml\", \"w\") as file:\n",
        "    yaml.dump(config, file, default_flow_style=False)"
      ],
      "metadata": {
        "id": "w-A-t3bM-B59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python  test.py --save-txt  --weights {weights_path}   --img 416 --data data.yaml"
      ],
      "metadata": {
        "id": "DdYxXS2CEEfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6G6zYgg_kzd"
      },
      "source": [
        "#Calculating the uncertinity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_images_nominees = nominateImagesToObtainUncer(main_dir+main_exp_dir+previous_iteration_folder+previousExpfName,expName, fromRandSize = number_of_random_images_from_training)"
      ],
      "metadata": {
        "id": "oGvjIF_xELsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python  detect.py --save-txt --save-conf --weights {weights_path}   --conf 0.05  --img 416 --source {main_dir+main_exp_dir+iteration_folder+expName+'_Nominees/images/'} --nosave\n"
      ],
      "metadata": {
        "id": "OIteZEtIEdC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyI6FlJ7_r68"
      },
      "source": [
        "#Weighing the uncertinities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OS28pGSZmv9t"
      },
      "outputs": [],
      "source": [
        "expfName = 'iteration'+str(iteration)+approach+weighting_approach+'_'+str(number_of_random_images_from_training)\n",
        "predFile = '/content/yolov7-AL/runs/detect/exp'\n",
        "wafile = '../runs/detect/val/results.txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previousFileList = glob.glob(main_dir+main_exp_dir+previous_iteration_folder+previousExpfName+'labels/*.txt')\n",
        "trainOnDUA = getUncertinitiesForDir(predFile, valAccuracyDirF =wafile, method='DUA',budget = int(500),weighting = 3,)"
      ],
      "metadata": {
        "id": "VDLhZ36CEqeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQeoo995AEZY"
      },
      "source": [
        "#Construct the new train set for the next iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd8EMvn8fkCJ"
      },
      "outputs": [],
      "source": [
        "newTraining(expfName,trainOnDUA,previousFileList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00ttH7fmm2Kf"
      },
      "outputs": [],
      "source": [
        "path_label = complete_iteration_folder+expfName+'/labels/'\n",
        "path_images = complete_iteration_folder+expfName+'/images/'\n",
        "\n",
        "wimg = glob.glob(complete_iteration_folder+wexpName+'_Nominees/images/*')\n",
        "wtxt = glob.glob(complete_iteration_folder+wexpName+'_Nominees/labels/*')\n",
        "for pth in wtxt:\n",
        "  namef = Path(pth).name[:-4]\n",
        "  src_path = complete_iteration_folder+wexpName+'_Nominees/labels/'+ namef+'.txt'\n",
        "  dst_path = path_label + namef+'.txt'\n",
        "  shutil.copy(src_path, dst_path)\n",
        "for pth in wimg:\n",
        "  namef = Path(pth).name[:-4]\n",
        "  src_path = complete_iteration_folder+wexpName+'_Nominees/images/'+ namef+'.jpg'\n",
        "  dst_path = path_images + namef+'.jpg'\n",
        "  shutil.copy(src_path, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw-P_q7Mm7FY"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cRsOwsQRh8-U",
        "H99K5e0QDxWk",
        "rOAJNyE0EM2d"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
